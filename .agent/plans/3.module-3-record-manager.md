# Module 3: Record Manager

## Context

Currently, uploading the same file twice creates duplicate chunks in `document_chunks` — the ingestion pipeline has no awareness of prior uploads. Module 3 adds a **Record Manager**: content hashing at the document and chunk level so the system can detect duplicates, skip redundant processing, and handle re-uploads of modified files cleanly.

**Complexity:** ⚠️ Medium

---

## Task 1: Database Migration

**File:** `supabase/migrations/00004_record_manager.sql`

Add content hashing support to the schema:

- Add `content_hash TEXT` column to `documents` (nullable for backward compat with existing rows)
- Add unique index on `(user_id, content_hash)` — dedup is per-user only
- Expand status check constraint to include `'duplicate'`:
  ```sql
  ALTER TABLE public.documents DROP CONSTRAINT documents_status_check;
  ALTER TABLE public.documents ADD CONSTRAINT documents_status_check
    CHECK (status IN ('pending', 'processing', 'completed', 'error', 'duplicate'));
  ```
- Add `content_hash TEXT` column to `document_chunks` (nullable, no unique constraint — tracking only for future incremental updates)

**Validate:** Apply migration with `supabase db reset`, confirm columns and constraint exist.

---

## Task 2: Hashing Utility Library

**File (new):** `server/lib/hashing.js`

Two functions using Node.js built-in `crypto`:
- `hashBuffer(buffer)` → SHA-256 hex digest of a Buffer (for file-level hashing)
- `hashString(text)` → SHA-256 hex digest of a string (for chunk-level hashing)

**File (new):** `server/tests/unit/hashing.test.js`

Test cases:
- Returns 64-char hex string
- Deterministic (same input → same output)
- Different inputs → different hashes
- `hashString('hello')` matches known SHA-256

**Validate:** `cd server && npm test -- --run tests/unit/hashing.test.js`

---

## Task 3: Upload Deduplication Logic

**File:** `server/routes/ingestion.js`

Modify `POST /api/ingestion/upload` to check for duplicates before storing/processing:

1. Compute `contentHash = hashBuffer(req.file.buffer)` immediately after receiving the file
2. **Check for exact duplicate** — query `documents` for matching `content_hash` + `user_id` (via user supabase client, which enforces RLS):
   ```js
   const { data: existing } = await supabase
     .from('documents')
     .select('*')
     .eq('content_hash', contentHash)
     .in('status', ['completed', 'processing', 'pending'])
     .maybeSingle();
   ```
3. **If duplicate found:** return `200` with `{ ...existing, duplicate: true }`. Skip storage upload and processing entirely.
4. **Check for same filename** (content changed) — query for existing doc with same filename:
   ```js
   const { data: sameNameDoc } = await supabase
     .from('documents')
     .select('id, storage_path, content_hash')
     .eq('filename', originalname)
     .in('status', ['completed', 'processing', 'pending'])
     .maybeSingle();
   ```
5. **If same filename with different hash:** delete old document (storage file + DB row, cascades to chunks) before proceeding with new upload.
6. **Normal flow:** include `content_hash: contentHash` in the document insert.

---

## Task 4: Chunk-Level Hashing in processDocument

**File:** `server/routes/ingestion.js`

In `processDocument()`, add `content_hash: hashString(chunk.content)` to each chunk row when building `chunkRows`. This is additive — no behavior change, just stores metadata for future incremental updates.

---

## Task 5: Integration Tests for Record Manager

**File:** `server/tests/integration/ingestion.test.js`

Add new test cases in a `describe('Record Manager')` block:

1. **Duplicate detection** — Mock hash-check query to return existing doc. Assert response is `200` with `duplicate: true`. Assert storage upload was NOT called.
2. **Re-upload with changed content** — Mock hash-check to return no match, filename-check to return existing doc with different hash. Assert old doc is deleted, then new doc is created with `content_hash`.
3. **New file includes content_hash** — Assert the document insert payload contains a 64-char hex `content_hash`.
4. **Chunk rows include content_hash** — Assert chunk insert includes `content_hash` field.

**Validate:** `cd server && npm test` — all tests pass.

---

## Task 6: Frontend — Duplicate Handling

**Files:**
- `client/src/components/DocumentCard.jsx` — Add `duplicate: 'secondary'` to `statusVariant` map
- `client/src/components/FileUpload.jsx` — After upload response, check `doc.duplicate === true`. If so, show a toast/alert: "File already uploaded (identical content)." and still call `onUploaded` so the list refreshes.

**Validate:** Manual browser test — upload a file, upload it again, see duplicate notification. Upload modified version of same filename, see old entry replaced.

---

## Task 7: Run Full Test Suite & Update Progress

- Run `cd server && npm test` — all tests pass (existing + new)
- Update `PROGRESS.md` with Module 3 tasks

---

## Key Files

| File | Action |
|------|--------|
| `supabase/migrations/00004_record_manager.sql` | Create |
| `server/lib/hashing.js` | Create |
| `server/routes/ingestion.js` | Modify |
| `server/tests/unit/hashing.test.js` | Create |
| `server/tests/integration/ingestion.test.js` | Modify |
| `client/src/components/DocumentCard.jsx` | Modify |
| `client/src/components/FileUpload.jsx` | Modify |
| `PROGRESS.md` | Modify |

## Verification

1. `cd server && npm test` — all tests pass
2. Browser test: upload file → completed. Upload same file → "duplicate" message, no reprocessing. Upload modified file with same name → old doc replaced, new doc processed.
