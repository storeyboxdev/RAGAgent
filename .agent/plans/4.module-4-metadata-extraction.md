# Module 4: Metadata Extraction

## Context

After Modules 1-3, documents are uploaded, chunked, embedded, and deduplicated — but retrieval is purely vector similarity. Module 4 adds LLM-powered structured metadata extraction during ingestion, enabling metadata-filtered retrieval. This teaches structured extraction with Zod, schema design, and metadata-enhanced search.

**Complexity:** ⚠️ **Medium** — touches DB, backend pipeline, retrieval, chat tool, and frontend, but each change is straightforward.

---

## Architecture Decision: Document-Level Metadata

Metadata is extracted at the **document level** (one LLM call per document), stored as JSONB on the `documents` table. Rationale:
- **Performance**: 1 LLM call per document vs N calls per chunk
- **Coherence**: Document-level metadata (topic, type, entities) is more meaningful than fragmented chunk-level metadata
- **Filtering model**: First find matching document IDs by metadata, then restrict vector search to those documents' chunks

Input to the LLM: first ~4000 characters of the document text.

---

## Tasks

### Task 1: Database Migration
**Create:** `supabase/migrations/00005_document_metadata.sql`

- Add `metadata jsonb` column to `documents` table
- Update status CHECK constraint to include `'extracting'` (new status shown while LLM extracts metadata)
- Update `match_document_chunks` function to accept optional `filter_document_ids uuid[] DEFAULT NULL` parameter — when provided, restrict search to chunks from those documents only. When NULL, behave as before (backward compatible).

**Reference:** Current status constraint is in `00004_record_manager.sql:12-13`, current RPC function is in `00002_module2_schema.sql:78-107`

**Validate:** `supabase db reset` succeeds, `metadata` column exists, RPC function accepts new param, passing NULL produces same results as before.

---

### Task 2: Metadata Schema + Extraction Library
**Create:** `server/lib/metadata.js`

- Define `DocumentMetadataSchema` with Zod:
  - `topic` (string) — primary topic/subject
  - `document_type` (enum: article, report, tutorial, documentation, email, memo, legal, academic, other)
  - `key_entities` (array of strings, max 10) — people, orgs, technologies, concepts
  - `summary` (string, max 500 chars) — 1-3 sentence summary
  - `language` (string) — primary language
- Export `extractMetadata(text)` function:
  - Truncate input to 4000 chars
  - Use `getLlmModel()` from `server/lib/lmstudio.js` → `model.respond()` with temperature 0.1
  - Prompt asks for JSON-only response matching the schema
  - Strip markdown code fences from response, parse JSON, validate with Zod
  - Return validated metadata object; throw on failure (caller handles)
- Export the schema for reuse in tests

**Validate:** Unit tests (Task 6).

---

### Task 3: Ingestion Pipeline Update
**Modify:** `server/routes/ingestion.js`

In `processDocument()` (line 147), after chunks are stored (line 195) and before the final status update (line 199):

1. Add `import { extractMetadata } from '../lib/metadata.js'` at top
2. Set status to `'extracting'` with chunk_count
3. Call `extractMetadata(text)` wrapped in try/catch — **non-fatal**: if extraction fails, log error and set `metadata = null`
4. Update final status to `'completed'` with both `chunk_count` and `metadata`

Updated pipeline: pending → processing → (chunk/embed/store) → extracting → (LLM metadata extraction) → completed

**Validate:** Upload a document, observe status transitions through to completed, verify `metadata` column is populated.

---

### Task 4: Retrieval Enhancement
**Modify:** `server/lib/retrieval.js`

Update `searchDocuments()` to accept optional `metadata_filter` in options:
- If `metadata_filter` has properties, query `documents` table using Supabase JSONB operators (`metadata->>key`) to find matching document IDs
- Pass those IDs as `filter_document_ids` to the `match_document_chunks` RPC
- If no documents match the filter, return `[]` immediately
- If filter query errors, fall through to unfiltered search (non-fatal)

**Validate:** Test filtered vs unfiltered search returns correct results.

---

### Task 5: Chat Tool Update
**Modify:** `server/routes/chat.js`

1. Update `search_documents` tool parameters (line 65-67) to add optional `metadata_filter` object with `topic` (string, optional) and `document_type` (enum, optional)
2. Update tool implementation (line 68) to pass `metadata_filter` through to `searchDocuments()`
3. Update tool description to mention metadata filtering capability
4. Update `SYSTEM_PROMPT` (line 11) to mention the LLM can filter by metadata when the user's question suggests a specific category

**Validate:** Ask questions like "search only tutorials about Python" and verify the LLM uses metadata_filter. Verify unfiltered queries still work.

---

### Task 6: Frontend Updates
**Modify:** `client/src/components/DocumentCard.jsx`

- Add `extracting` to `statusVariant` map (line 7-13) with `'outline'` variant
- Add spinner for `extracting` status alongside `processing` (line 30)
- After the status/chunks area (line 39), display metadata when present:
  - `document_type` as a Badge
  - `topic` as small muted text

**Modify:** `client/src/components/ToolCallIndicator.jsx`

- When `toolCall.arguments?.metadata_filter` exists, display active filters next to "Searching documents..." text (line 17)

**Validate:** Upload a doc, see extracting status with spinner, then see metadata badges on completion. Trigger filtered search in chat, see filter info in tool call indicator.

---

### Task 7: Tests
**Create:** `server/tests/unit/metadata.test.js`

Unit tests for metadata extraction:
- Valid LLM response → correctly parsed metadata object
- Markdown-wrapped JSON → stripped and parsed
- Invalid `document_type` → throws (Zod validation)
- Malformed JSON → throws
- Zod schema validates correct structure
- Zod schema rejects missing required fields
- Text truncated to max extraction chars

Mock pattern: mock `@lmstudio/sdk`, mock `../../lib/lmstudio.js` (same pattern as `server/tests/unit/hashing.test.js` and `server/tests/integration/ingestion.test.js`)

**Modify:** `server/tests/integration/ingestion.test.js`

Add `describe('Metadata Extraction')` block:
- Mock `../../lib/metadata.js` with `extractMetadata`
- Test: processDocument calls extractMetadata and includes metadata in final update
- Test: extractMetadata failure is non-fatal — document still reaches completed with metadata: null
- Test: status transitions include 'extracting'

Follow existing test patterns (lines 231-268 for async processing tests).

**Validate:** `cd server && npm test` — all existing + new tests pass.

---

### Task 8: Update PROGRESS.md

Add Module 4 section with all tasks marked complete.

---

## File Summary

| File | Action |
|------|--------|
| `supabase/migrations/00005_document_metadata.sql` | Create |
| `server/lib/metadata.js` | Create |
| `server/routes/ingestion.js` | Modify |
| `server/lib/retrieval.js` | Modify |
| `server/routes/chat.js` | Modify |
| `client/src/components/DocumentCard.jsx` | Modify |
| `client/src/components/ToolCallIndicator.jsx` | Modify |
| `server/tests/unit/metadata.test.js` | Create |
| `server/tests/integration/ingestion.test.js` | Modify |
| `PROGRESS.md` | Modify |

## Execution Order

1 → 2 → 3 → 4 → 5 → 6 → 7 → 8

## Verification

1. `cd server && npm test` — all tests pass
2. `supabase db reset` — migration applies cleanly
3. Upload a text file → observe pending → processing → extracting → completed
4. Verify metadata populated on document (check via API or DB)
5. Chat: ask a question → LLM uses search_documents without filters → works as before
6. Chat: ask "search only tutorials about X" → LLM uses metadata_filter → filtered results
7. Frontend: DocumentCard shows document_type badge and topic
8. Frontend: ToolCallIndicator shows active metadata filters

## Risks

1. **LMStudio `model.respond()` return type**: May return string or object. Implementation handles both with fallback parsing.
2. **Supabase JSONB filtering syntax**: `metadata->>key` should work with `.eq()` and `.ilike()`. If not, fallback to raw filter.
3. **Local LLM JSON quality**: Low temperature (0.1) + explicit JSON-only prompt + code-fence stripping + Zod validation. Extraction failure is non-fatal.
