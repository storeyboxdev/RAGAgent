# Build: Module 2 â€” BYO Retrieval + Memory

## Context
Module 1 delivered a working chat app using OpenAI's Responses API. Module 2 replaces that with the standard Chat Completions API (supporting LMStudio/Ollama), adds document ingestion with vector search, and implements tool calling so the LLM can retrieve relevant document chunks during conversation.

**Architectural Decision:** Option A â€” completely remove Responses API code, single Chat Completions pattern going forward.

**LLM Providers:** Configurable via env vars to support both LMStudio and Ollama (any OpenAI-compatible endpoint).

**Complexity:** ðŸ”´ Complex â€” 10 tasks spanning schema migration, LLM swap, ingestion pipeline, vector search, tool calling, and frontend updates.

---

## Execution Order

### Task 1: Database Migration
- Create `supabase/migrations/00002_module2_schema.sql`
- Drop `openai_response_id` from `threads`
- Create `documents` table (id, user_id, filename, file_type, file_size, storage_path, status, error_message, chunk_count, timestamps) with RLS
- Create `document_chunks` table (id, document_id, user_id, content, chunk_index, embedding vector(768)) with RLS + HNSW index
- Create `match_document_chunks` Postgres function for vector similarity search
- Add `documents` to Supabase Realtime publication
- Create `documents` storage bucket with user-scoped RLS policies
- **Validate:** `npx supabase db reset` succeeds, tables visible in Studio

### Task 2: Environment Config + LLM Client Refactor
- Update `.env.example` with: `LLM_BASE_URL`, `LLM_MODEL`, `LLM_API_KEY`, `EMBEDDING_BASE_URL`, `EMBEDDING_MODEL`, `EMBEDDING_DIMENSIONS`
- Rewrite `server/lib/openai.js` â€” export `llmClient`, `embeddingClient`, `LLM_MODEL`, `EMBEDDING_MODEL`, `EMBEDDING_DIMENSIONS` (configurable base URLs, fallback to OpenAI)
- Remove `OPENAI_API_KEY` references (replaced by `LLM_API_KEY`)
- **Validate:** Server boots, health returns 200

### Task 3: Replace Responses API with Chat Completions
- Rewrite `server/routes/chat.js` â€” use `llmClient.chat.completions.create()` with stateless messages array
- Load full message history from thread JSONB, append user message, send all as `messages` param
- Stream `delta.content` chunks as SSE `text_delta` events
- Remove all `openai_response_id` references, remove inner manual Laminar span (auto-instrumentation works for Chat Completions)
- Keep outer `observe()` wrapper with `input`/`setSpanOutput`
- **Validate:** Send message â†’ response streams; send follow-up â†’ context preserved; refresh â†’ messages persist; Laminar traces show auto-instrumented OpenAI spans

### Task 4: Chunking Library
- Create `server/lib/chunking.js` â€” `chunkText(text, { chunkSize, chunkOverlap })`
- Split on paragraph boundaries (`\n\n`), fall back to sentence boundaries, then character count
- Overlap by including trailing content from previous chunk
- Returns `[{ content, chunkIndex }]`
- **Validate:** Test with sample text, verify chunk sizes, overlap, no content loss

### Task 5: Embedding Library
- Create `server/lib/embeddings.js` â€” `generateEmbeddings(texts)` and `generateEmbedding(text)`
- Uses `embeddingClient.embeddings.create()` with configured model/dimensions
- **Validate:** With LMStudio/Ollama running `nomic-embed-text`, call `generateEmbedding("test")` â†’ returns 768-float array

### Task 6: Ingestion Backend
- Create `server/routes/ingestion.js`
- Add `multer` dependency to `server/package.json`
- Mount at `/api/ingestion` in `server/index.js`
- `POST /api/ingestion/upload` â€” receive file via multer, store in Supabase Storage at `{user_id}/{doc_id}/{filename}`, insert `documents` row, kick off async processing (don't await)
- Async `processDocument()`: update statusâ†’processing, download file, chunk, embed, store chunks, update statusâ†’completed (or error)
- `GET /api/ingestion/documents` â€” list user's documents
- `DELETE /api/ingestion/documents/:id` â€” delete doc + storage file
- **Validate:** Upload .txt via curl â†’ document progresses pendingâ†’processingâ†’completed, chunks appear with embeddings

### Task 7: Retrieval + Tool Calling in Chat
- Create `server/lib/retrieval.js` â€” `searchDocuments(query, userId, { limit, threshold })`
- Calls `match_document_chunks` RPC via `supabaseAdmin`
- Update `server/routes/chat.js` â€” add `tools` array with `search_documents` function definition
- Implement tool call loop: if LLM returns `tool_calls`, execute search, send results back as `tool` messages, call LLM again for final answer
- Add SSE events: `tool_call` (name, arguments) and `tool_result` (name, chunks)
- Store full message exchange (including tool calls) in JSONB
- Update system prompt to mention document search capability
- **Validate:** Upload doc â†’ ask about its content â†’ LLM calls tool â†’ retrieves chunks â†’ answers from docs

### Task 8: Ingestion Frontend
- Create `client/src/components/AppLayout.jsx` â€” shared layout with Chat/Documents tab nav + user/signout in header
- Create `client/src/pages/IngestionPage.jsx` â€” main ingestion view
- Create `client/src/components/FileUpload.jsx` â€” drag-and-drop upload zone (.txt only)
- Create `client/src/components/DocumentList.jsx` â€” fetch + display documents
- Create `client/src/components/DocumentCard.jsx` â€” filename, status badge, chunk count, delete
- Update `client/src/App.jsx` â€” add view state (chat/documents), wrap in AppLayout
- Update `client/src/pages/ChatPage.jsx` â€” adjust layout for AppLayout (remove h-screen)
- Update `client/src/components/ThreadList.jsx` â€” remove sign-out (moved to AppLayout)
- **Validate:** Navigate to Documents tab, upload .txt, see doc appear, switch to Chat tab

### Task 9: Realtime Ingestion Status
- Update `client/src/components/DocumentList.jsx` â€” subscribe to Supabase Realtime `postgres_changes` on `documents` table filtered by user_id
- Status updates appear live without polling
- **Validate:** Upload file â†’ watch status change pendingâ†’processingâ†’completed in real time

### Task 10: Tool Call Display in Chat UI
- Create `client/src/components/ToolCallIndicator.jsx` â€” "Searching documents..." with expandable chunk results
- Update `client/src/pages/ChatPage.jsx` â€” parse `tool_call` and `tool_result` SSE events
- Update `client/src/components/ChatMessages.jsx` â€” render ToolCallIndicator between messages
- **Validate:** Ask about uploaded doc â†’ see search indicator â†’ see retrieved chunks (collapsible) â†’ see final answer

---

## Key Files Modified/Created

**Backend (server/):**
- `lib/openai.js` â€” REWRITE (configurable dual clients)
- `lib/chunking.js` â€” NEW
- `lib/embeddings.js` â€” NEW
- `lib/retrieval.js` â€” NEW
- `routes/chat.js` â€” REWRITE (Chat Completions + tool calling)
- `routes/ingestion.js` â€” NEW
- `index.js` â€” MODIFY (mount ingestion route)
- `package.json` â€” MODIFY (add multer)

**Frontend (client/src/):**
- `App.jsx` â€” MODIFY (view routing)
- `components/AppLayout.jsx` â€” NEW
- `pages/IngestionPage.jsx` â€” NEW
- `components/FileUpload.jsx` â€” NEW
- `components/DocumentList.jsx` â€” NEW
- `components/DocumentCard.jsx` â€” NEW
- `components/ToolCallIndicator.jsx` â€” NEW
- `pages/ChatPage.jsx` â€” MODIFY (tool call events, layout)
- `components/ChatMessages.jsx` â€” MODIFY (tool indicators)
- `components/ThreadList.jsx` â€” MODIFY (remove sign-out)

**Database:**
- `supabase/migrations/00002_module2_schema.sql` â€” NEW

**Config:**
- `.env.example` â€” MODIFY (new LLM/embedding vars)

---

## Verification (End-to-End)
1. `npx supabase db reset` â€” migration applies cleanly
2. Start LMStudio/Ollama with a chat model + `nomic-embed-text`
3. `npm run dev` â€” both server and client start
4. Sign in â†’ Chat tab â†’ send message â†’ response streams via Chat Completions
5. Documents tab â†’ upload .txt file â†’ watch status go pendingâ†’processingâ†’completed in real time
6. Chat tab â†’ ask about uploaded document â†’ see "Searching documents..." â†’ see answer citing document content
7. Refresh page â†’ messages and documents persist
8. Check Laminar â†’ traces show auto-instrumented chat.completions spans

## Progress Tracking
Update `PROGRESS.md` after each task with `[ ]` â†’ `[-]` â†’ `[x]`.
